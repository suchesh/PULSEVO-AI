# ============================================
# rag.py

# ============================================
# Smart Real-Time RAG Assistant for Company PDFs and Dashboard Queries
# - Place PDFs in the SAME folder as this file.
# - Embeds on first run, uses Groq LLM for context-driven, natural chat.
# ============================================

import os
import uuid
import chromadb
from pathlib import Path
import numpy as np
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
from langchain_groq import ChatGroq
from dotenv import load_dotenv


load_dotenv()
GROQ_KEY = os.getenv("GROQ_KEY")

# =============================
# EMBEDDING MANAGER
# =============================
class EmbeddingManager:
    """Handles text embedding using SentenceTransformer."""
    def __init__(self):
        print("üîπ Loading Embedding Model: all-MiniLM-L6-v2")
        self.model = SentenceTransformer("all-MiniLM-L6-v2")

    def embed(self, texts):
        print(f"üß† Generating embeddings for {len(texts)} chunks...")
        return np.array(self.model.encode(texts, show_progress_bar=False))


# =============================
# VECTOR DATABASE MANAGER
# =============================
class VectorStore:
    """Handles persistent vector storage and search using ChromaDB."""

    def __init__(self):
        os.makedirs("./vector_store", exist_ok=True)
        self.client = chromadb.PersistentClient(path="./vector_store")
        self.col = self.client.get_or_create_collection(name="company_docs")
        print(f"üì¶ DB Loaded: {self.col.count()} records")

    def add(self, docs, embeds):
        ids = [f"doc_{uuid.uuid4()}" for _ in docs]
        self.col.add(
            ids=ids,
            documents=[d.page_content for d in docs],
            embeddings=embeds.tolist(),
            metadatas=[d.metadata for d in docs]
        )
        print(f"‚úÖ Added {len(docs)} chunks to Vector DB. Now: {self.col.count()}")

    def search(self, q_embed, k=3):
        """Query the vector store for top-k similar chunks."""
        return self.col.query(query_embeddings=[q_embed.tolist()], n_results=k)


# =============================
# RAG (Retrieve & Generate) PIPELINE
# =============================
class RAG:
    """Main RAG logic: load PDFs, embed, search, and generate conversational answers."""

    def __init__(self, api_key):
        self.embedder = EmbeddingManager()
        self.db = VectorStore()
        self.llm = ChatGroq(api_key=api_key, model="groq/compound-mini")

    # ---------------------------
    # Build Knowledge Base
    # ---------------------------
    def build(self):
        """Load PDFs, split into chunks, embed, and store in ChromaDB."""
        pdfs = list(Path(".").glob("*.pdf"))
        if not pdfs:
            print("‚ö†Ô∏è No PDFs found in folder.")
            return

        docs = []
        for pdf in pdfs:
            loader = PyMuPDFLoader(str(pdf))
            loaded = loader.load()
            for d in loaded:
                d.metadata["source_file"] = pdf.name
            docs.extend(loaded)
            print(f"‚úÖ Loaded {len(loaded)} pages from {pdf.name}")

        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        chunks = splitter.split_documents(docs)

        embeds = self.embedder.embed([c.page_content for c in chunks])
        self.db.add(chunks, embeds)
        print("üöÄ RAG setup complete.")

    # ---------------------------
    # ASK A QUESTION (INTERACTIVE CHAT)
    # ---------------------------
    def ask(self, query, k=3):
        """Generate a conversational, real-time answer aligned with context."""
        print(f"\nüîé Query: {query}")

        # Create query embedding
        q_embed = self.embedder.embed([query])[0]
        res = self.db.search(q_embed, k)

        # Gather context (if found)
        context = ""
        if res.get("documents") and res["documents"][0]:
            context = "\n\n".join(res["documents"][0])

        # ==============================
        # HIGHLY QUALIFIED CONVERSATIONAL PROMPT
        # ==============================
        prompt = f"""
You are **SAI**, a world-class AI corporate assistant that speaks naturally like a human.
Your tone is friendly, confident, and professional ‚Äî never robotic or verbose.

üéØ **Your Mission**
- Act like a real-time chat assistant.
- Use short sentences, clean structure, and natural flow.
- Make replies easy to scan ‚Äî use line breaks and bullet points where helpful.
- Always connect your answers to the retrieved dashboard or company context if relevant.
- If question is irrelevant, answer politely and redirect.

---

üß≠ **Behavior Framework**

1Ô∏è‚É£ **Greetings / Casual Talk**  
‚Üí Respond warmly in 1‚Äì2 lines.  
Example: ‚Äúüëã Hey there! Great to see you. What can I help you with today?‚Äù

2Ô∏è‚É£ **Relevant or Dashboard Queries (Data / Stats / Company Info)**  
‚Üí Use the provided context below.  
‚Üí Provide a short **summary + key points (bullets or short lines)**.  
‚Üí Include numbers naturally (no raw dumps).  
‚Üí Conclude politely (e.g., ‚ÄúWould you like me to expand on that?‚Äù)

3Ô∏è‚É£ **Irrelevant / Personal Queries**  
‚Üí Decline gently.  
Example: ‚Äúüòä I‚Äôm designed to focus on company and dashboard insights only. Want me to show related analytics instead?‚Äù

4Ô∏è‚É£ **Formatting Rules**  
- Use a maximum of **5 short lines**.  
- Add **line breaks** between ideas.  
- Use **bullets or dashes** for clarity.  
- Never echo the user‚Äôs question verbatim.

---

üìä **Relevant Context:**
{context if context else "No matching company or dashboard data found."}

üí¨ **User Message:**
"{query}"

---

Now, respond as **SAI**, your intelligent corporate assistant.
Keep tone human, helpful, and concise.
Avoid essays ‚Äî reply like a professional who explains clearly in a chat.
"""

        reply = self.llm.invoke(prompt)
        return reply.content.strip()







# from langchain_groq import ChatGroq

# # Load environment
# load_dotenv()
# GROQ_KEY = os.getenv("GROQ_KEY")
# MONGO_URI = os.getenv("MONGO_URI")   # ‚úÖ Add your MongoDB URI in .env
# MONGO_DB = os.getenv("MONGO_DB", "company_db")
# MONGO_COLLECTION = os.getenv("MONGO_COLLECTION", "company_data")


# # =============================
# # EMBEDDING MANAGER
# # =============================
# class EmbeddingManager:
#     """Handles text embedding using SentenceTransformer."""

#     def __init__(self):
#         print("üîπ Loading Embedding Model: all-MiniLM-L6-v2")
#         self.model = SentenceTransformer("all-MiniLM-L6-v2")

#     def embed(self, texts):
#         print(f"üß† Generating embeddings for {len(texts)} chunks...")
#         return np.array(self.model.encode(texts, show_progress_bar=False))


# # =============================
# # VECTOR DATABASE MANAGER
# # =============================
# class VectorStore:
#     """Handles persistent vector storage and search using ChromaDB."""

#     def __init__(self):
#         os.makedirs("./vector_store", exist_ok=True)
#         self.client = chromadb.PersistentClient(path="./vector_store")
#         self.col = self.client.get_or_create_collection(name="company_docs")
#         print(f"üì¶ DB Loaded: {self.col.count()} records")

#     def add(self, docs, embeds):
#         ids = [f"doc_{uuid.uuid4()}" for _ in docs]
#         self.col.add(
#             ids=ids,
#             documents=docs,
#             embeddings=embeds.tolist(),
#             metadatas=[{"source": "mongodb"} for _ in docs]
#         )
#         print(f"‚úÖ Added {len(docs)} docs to Vector DB. Now total: {self.col.count()}")

#     def search(self, q_embed, k=3):
#         """Query the vector store for top-k similar chunks."""
#         return self.col.query(query_embeddings=[q_embed.tolist()], n_results=k)


# # =============================
# # RAG (Retrieve & Generate)
# # =============================
# class RAG:
#     """Main RAG logic: fetch from MongoDB, embed, search, and generate conversational answers."""

#     def __init__(self, api_key):
#         self.embedder = EmbeddingManager()
#         self.db = VectorStore()
#         self.llm = ChatGroq(api_key=api_key, model="groq/compound-mini")
#         self.mongo_client = MongoClient(MONGO_URI)
#         self.mongo_collection = self.mongo_client[MONGO_DB][MONGO_COLLECTION]

#     # ---------------------------
#     # Build Knowledge Base from MongoDB
#     # ---------------------------
#     def build(self):
#         """Fetch data from MongoDB, chunk, embed, and store in ChromaDB."""
#         print("üì° Fetching documents from MongoDB...")

#         docs = []
#         for doc in self.mongo_collection.find({}, {"_id": 0}):
#             text = " ".join(str(v) for v in doc.values() if isinstance(v, (str, int, float)))
#             if text.strip():
#                 docs.append(text)

#         if not docs:
#             print("‚ö†Ô∏è No MongoDB documents found.")
#             return

#         print(f"‚úÖ Fetched {len(docs)} records from MongoDB.")

#         # Optional: split into smaller chunks for better embeddings
#         splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
#         chunks = []
#         for text in docs:
#             parts = splitter.split_text(text)
#             chunks.extend(parts)

#         print(f"üß© Split into {len(chunks)} text chunks.")

#         # Create embeddings
#         embeds = self.embedder.embed(chunks)
#         self.db.add(chunks, embeds)
#         print("üöÄ MongoDB-based RAG setup complete.")

#     # ---------------------------
#     # ASK A QUESTION
#     # ---------------------------
#     def ask(self, query, k=3):
#         """Generate an intelligent answer using MongoDB + Chroma context."""
#         print(f"\nüîé Query: {query}")

#         # Create query embedding
#         q_embed = self.embedder.embed([query])[0]
#         res = self.db.search(q_embed, k)

#         # Build context
#         context = ""
#         if res.get("documents") and res["documents"][0]:
#             context = "\n\n".join(res["documents"][0])

#         # -----------------------------
#         # Prompt
#         # -----------------------------
#         prompt = f"""
# You are **SAI**, a professional AI assistant that summarizes and explains company data clearly.
# Your responses are conversational, factual, and to the point.


# üìä **Context:**
# {context if context else "No relevant data found in company database."}

# üí¨ **User Query:**
# {query}

# Now, respond in a clear and human tone.
# """

#         reply = self.llm.invoke(prompt)
#         return reply.content.strip()


# # =============================
# # GLOBAL INSTANCE (For FastAPI)
# # =============================
# rag_instance = RAG(GROQ_KEY)

# # If Chroma DB empty, build from Mongo
# if rag_instance.db.col.count() == 0:
#     rag_instance.build()
# else:
#     print("‚úÖ Using existing vector DB from Mongo data.")

